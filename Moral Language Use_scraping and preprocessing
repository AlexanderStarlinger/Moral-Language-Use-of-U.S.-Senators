{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "## imports\n\nimport snscrape.modules.twitter as sntwitter\nimport pandas as pd\nimport os\nimport datetime\nimport numpy as np\nimport seaborn as sns\nimport regex as re\nfrom nltk.corpus import wordnet as wn\nimport itertools\nimport nltk\nimport contractions\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport ast\nfrom glob import glob\nimport collections\nfrom scipy import stats\nimport preprocessor as p\n\nlemma = WordNetLemmatizer()\nstop_words = set(stopwords.words(\"english\"))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## preprocessing pipeline \n\ndef clean_url(input):\n    output = re.sub(r\"http\\S+\", \"\", input)\n    return output\n\n\ndef fix_contraction(input):\n    output = contractions.fix(input)\n    return output\n\n\ndef clean_non_alphanumeric(input):\n    output = re.sub(r\"[^a-zA-Z0-9]\", \" \", input)\n    return output\n\n\ndef clean_tokenization(input):\n    output = nltk.word_tokenize(input)\n    return output\n\n\ndef clean_stopwords(input):\n    output = [item for item in input if item not in stop_words]\n    return output\n\n\ndef clean_lowercase(input):\n    output = str(input).lower()\n    return output\n\n\ndef clean_lemmatization(input):\n    output = [lemma.lemmatize(word=w, pos=\"v\") for w in input]\n    return output\n\n\ndef clean_length(input):\n    output = [word for word in input if len(word) > 2]\n    return output\n\n\ndef convert_to_string(input):\n    output = \" \".join(input)\n    return output\n\n\ndef preprocessing(text, remove_stopwords=True):\n    \"\"\"\n    Preprocessing pipeline.\n    \"\"\"\n    text = clean_url(text)\n    text = fix_contraction(text)\n    text = clean_non_alphanumeric(text)\n    text = clean_lowercase(text)\n    text = clean_tokenization(text)\n    if remove_stopwords:\n        text = clean_stopwords(text)\n    text = clean_lemmatization(text)\n    text = clean_length(text)\n    text = convert_to_string(text)\n    return text",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def compute_frequencies(list_string, list_of_categories):\n    \"\"\"\n    Function to calculate word frequencies.\n\n    \"\"\"\n\n    global apply_counter1\n\n    tokenized_text = ast.literal_eval(list_string)\n    word_counter = collections.Counter(tokenized_text)\n    total_words = len(tokenized_text)\n\n    list_of_category_frequencies = []\n    for category in list_of_categories:\n        category_count = 0.0\n\n        for word in category:\n            category_count += int(word_counter[word])\n\n        if total_words != 0 and category_count > 0:\n            category_frequency = category_count / total_words\n        else:\n            category_frequency = float(\"nan\")\n        list_of_category_frequencies += [category_frequency]\n\n    apply_counter1 += 1\n\n    return list_of_category_frequencies\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## setwd\n\nos.chdir(\"C:/Users/Alex/Desktop/Uni/Master Psychologie/WS 22/SE CSS/Final Project/project/congress twitter handles\")\n\n\n# make lists of senators; note on periods:\n    ## 114: since:2015-01-01 until:2016-12-31\n    ## 115: since:2017-01-01 until:2018-12-31\n    ## 116: since:2019-01-01 until:2020-12-31\n    ## 117: since:2021-01-01 until:2022-12-31\n\ndf_114 = pd.read_excel(\"114th congress list.xlsx\")\ndf_115 = pd.read_excel(\"115th congress list.xlsx\")\ndf_116 = pd.read_excel(\"116th congress list.xlsx\")\ndf_117 = pd.read_excel(\"117th congress list.xlsx\")\n\nsenators_114 = list(df_114[\"Handle\"])\nsenators_115 = list(df_115[\"Handle\"])\nsenators_116 = list(df_116[\"Handle\"])\nsenators_117 = list(df_117[\"Handle\"])\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Scraping\n\n## Limit maxTweets\n\nmaxTweets = 1500\n\n## 114 congress\n\n### Creating list to append tweet data to\n\ntweets_list114 = []\n\n### scrape \n\nfor sen in senators_114:\n\n### Using TwitterSearchScraper to scrape data \n    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'from:{sen} since:2015-01-01 until:2016-12-31').get_items()):\n        if i>=maxTweets:\n            break\n        tweets_list114.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n\n### Creating a dataframe from the tweets list above \ntweets_114 = pd.DataFrame(tweets_list114, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n\n### convert Datetime and make year variable\n\ntweets_114[\"Datetime\"] = pd.to_datetime(tweets_114[\"Datetime\"])\ntweets_114[\"Year\"] = tweets_114[\"Datetime\"].dt.year\n## 115 congress\n\n# Creating list to append tweet data to\n\ntweets_list115 = []\n\n# scrape that shit\n\nfor sen in senators_115:\n\n# Using TwitterSearchScraper to scrape data \n    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'from:{sen} since:2017-01-01 until:2018-12-31').get_items()):\n        if i>=maxTweets:\n            break\n        tweets_list115.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n\n# Creating a dataframe from the tweets list above \ntweets_115 = pd.DataFrame(tweets_list115, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n\n# convert Datetime and make year variable\n\ntweets_115[\"Datetime\"] = pd.to_datetime(tweets_115[\"Datetime\"])\ntweets_115[\"Year\"] = tweets_115[\"Datetime\"].dt.year\n\n\n## 116 congress\n\n# Creating list to append tweet data to\n\ntweets_list116 = []\n\n# scrape that shit\n\nfor sen in senators_116:\n\n# Using TwitterSearchScraper to scrape data \n    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'from:{sen} since:2019-01-01 until:2020-12-31').get_items()):\n        if i>=maxTweets:\n            break\n        tweets_list116.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n\n# Creating a dataframe from the tweets list above \ntweets_116 = pd.DataFrame(tweets_list116, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n\n# convert Datetime and make year variable\n\ntweets_116[\"Datetime\"] = pd.to_datetime(tweets_116[\"Datetime\"])\ntweets_116[\"Year\"] = tweets_116[\"Datetime\"].dt.year\n\n\n## 117 congress\n\n# Creating list to append tweet data to\n\ntweets_list117 = []\n\n# scrape that shit\n\nfor sen in senators_117:\n\n# Using TwitterSearchScraper to scrape data \n    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'from:{sen} since:2021-01-01 until:2022-12-31').get_items()):\n        if i>=maxTweets:\n            break\n        tweets_list117.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n\n# Creating a dataframe from the tweets list above \ntweets_117 = pd.DataFrame(tweets_list117, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n\n# convert Datetime and make year variable\n\ntweets_117[\"Datetime\"] = pd.to_datetime(tweets_117[\"Datetime\"])\ntweets_117[\"Year\"] = tweets_117[\"Datetime\"].dt.year",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# export to csv\n\nos.chdir(\"C:/Users/Alex/Desktop/Uni/Master Psychologie/WS 22/SE CSS/Final Project/project/data\")\n\ntweets_114.to_csv(\"tweets_114_full.csv\")\ntweets_115.to_csv(\"tweets_115_full.csv\")\ntweets_116.to_csv(\"tweets_116_full.csv\")\ntweets_117.to_csv(\"tweets_117_full.csv\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# import csvs\n\n# setwd\n\nos.chdir(\"C:/Users/Alex/Desktop/Uni/Master Psychologie/WS 22/SE CSS/Final Project/project/data\")\n\n\ntweets_114 = pd.read_csv(\"tweets_114_full.csv\")\ntweets_115 = pd.read_csv(\"tweets_115_full.csv\")\ntweets_116 = pd.read_csv(\"tweets_116_full.csv\")\ntweets_117 = pd.read_csv(\"tweets_117_full.csv\")\n\ndfs = [tweets_114, tweets_115, tweets_116, tweets_117]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# concatenate df\n\n## append congress variable for each df\n\nfor i, df in enumerate(dfs):\n    \n    df[\"congress_ID\"] = np.full(len(df), i)\n    \n## concatenate\n\ntweets_df = pd.concat([tweets_114, tweets_115, tweets_116, tweets_117]).reset_index()\n\n# delete Datetime, Tweet Id, and variable created by merge (congress variable already identifier)\n\ntweets_df = tweets_df.drop(labels = [\"Datetime\", \"Tweet Id\", \"Unnamed: 0\"], axis = 1)\n\n\n## append political affiliation (in a really dumb way...)\n\ndemocrats = [\"SenFeinstein\", \"SenatorBoxer\", \"SenBennetCO\", \"ChrisMurphyCT\", \"SenatorCarper\", \"ChrisCoons\", \"SenBillNelson\",\n             \"brianschatz\", \"maziehirono\", \"SenatorDurbin\", \"SenatorBarb\", \"senmarkey\", \"SenWarren\", \"RepGaryPeters\",\n             \"SenStabenow\", \"amyklobuchar\", \"clairecmc\", \"SenatorReid\", \"SenatorShaheen\", \"CoryBooker\", \"SenatorMenendez\",\n             \"MartinHeinrich\", \"SenatorTomUdall\", \"SenSchumer\", \"SenGillibrand\", \"SenatorHeitkamp\", \"SenSherrodBrown\",\n             \"RonWyden\", \"SenJeffMerkley\", \"SenJackReed\", \"SenWhitehouse\", \"SenatorLeahy\", \"SenatorSanders\",\"timkaine\",\n             \"Sen_JoeManchin\", \"SenatorBaldwin\", \"SenDougJones\", \"SenKamalaHarris\", \"SenBlumenthal\", \"SenDuckworth\", \n             \"SenDonnelly\",\"ChrisVanHollen\", \"SenBobCasey\", \"MarkWarner\", \"PattyMurray\", \"SenatorCantwell\", \"SenatorSinema\",\n             \"SenatorBennet\", \"SenBrianSchatz\", \"SenMarkey\", \"SenJackyRosen\", \"gillibrandny\", \"SenSanders\", \"SenMarkKelly\",\n             \"SenAlexPadilla\", \"SenatorHick\", \"SenMurphyOffice\", \"SenatorWarnock\", \"SenOssoff\", \"SenAmyKlobuchar\",\n             \"SenBooker\", \"SenatorLujan\"]\n\n\n# append \n\n\npol_party = []\n\nfor i in tweets_df[\"Username\"]:\n    if i in democrats:\n        pol_party.append(\"D\")\n    else:\n        pol_party.append(\"R\")\n\ntweets_df[\"party\"] = pol_party\n\n\ntweets_df[\"Username\"][tweets_df[\"party\"] == \"D\"] #cool\n\n\n# export to csv\n\nos.chdir(\"C:/Users/Alex/Desktop/Uni/Master Psychologie/WS 22/SE CSS/Final Project/project/data\")\n\ntweets_df.to_csv(\"tweets_df_full.csv\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# import df\n\n## setwd\n\nos.chdir(\"C:/Users/Alex/Desktop/Uni/Master Psychologie/WS 22/SE CSS/Final Project/project/data\")\n\ntweets_df = pd.read_csv(\"tweets_df_full.csv\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# clean\n\n## 1) tweet-preprocessor\n\n\ntweets_df[\"Text_processed\"] = tweets_df[\"Text\"].apply(p.clean)\n\n\n## 2) own pipeline (classes)\n\ntweets_df[\"Text_processed\"] = tweets_df[\"Text_processed\"].apply(preprocessing)\n\ntweets_df[\"Text_processed\"] = tweets_df[\"Text_processed\"].apply(lambda txt: str(txt.split(\" \")))\n\n## delete text\n\ntweets_df = tweets_df.drop([\"Text\", \"Unnamed: 0\"], axis = 1)\n\n# export to csv\n\nos.chdir(\"C:/Users/Alex/Desktop/Uni/Master Psychologie/WS 22/SE CSS/Final Project/project/data\")\n\ntweets_df.to_csv(\"tweets_preprocessed_full.csv\")\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# import processed df\n\nos.chdir(\"C:/Users/Alex/Desktop/Uni/Master Psychologie/WS 22/SE CSS/Final Project/project/data\")\n\ntweets_processed = pd.read_csv(\"tweets_preprocessed_full.csv\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# BUILD OUTCOMES\n\n## define dictionaries (list_of_categories)\n\n\nharm_virtue = [r\"safe.*\", r\"peace.*\", r\"compassion.*\", r\"empath.*\", r\"sympath.*\", \"care\", r\"protect.*\", \n                \"shield\", \"shelter\", \"amity\", r\"secur.*\", r\"benefit.*\", r\"defen.*\", r\"guard.*\", \"preserve\"]\n\nharm_vice = [r\"harm.*\", r\"suffer.*\", \"war\",\t\"wars\", r\"warl.*\", \"warring\", r\"fight.*\", r\"violen.*\", r\"hurt.*\", \"kill\", \"kills\", r\"killer.*\", \"killed\", \"killing\",r\"endanger.*\",\n             r\"cruel.*\",r\"brutal.*\",r\"abuse.*\", r\"damag.*\",r\"ruin.*\",\"ravage\",r\"detriment.*\",r\"crush.*\",r\"attack.*\",\n            r\"annihilate.*\",\"destroy\",\"stomp\",r\"abandon.*\",\"spurn\",\"impair\",\"exploit\", \"exploits\",\"exploited\",\"exploiting\",r\"wound.*\"]\n\n\nfairness_virtue = ['fair', 'fairly', 'fairness', r'fair.*', r'fairmind.*', 'fairplay', r'equal.*', 'justice', 'justness', r'justifi.*', r'reciproc.*',r'impartial.*', r'egalitar.*', \n                   'rights', 'equity', 'equivalent', r'unbias.*', 'tolerant', 'equable', r'balance.*', 'homologous', r'unprejudice.*', 'reasonable', 'constant', r'honest.*']\n\n\nfairness_vice = [r'unfair.*', r'unequal.*', r'bias.*', r'unjust.*', r'injust.*', r'bigot.*', r'discriminat.*', r'disproportion.*', 'inequitable', r'prejud.*', 'dishonest', 'unscrupulous', \n                 'dissociate', 'preference', 'favoritism', r'segregat.*', 'exclusion', r'exclud.*']\n\ningroup_virtue = ['together', r'nation.*', r'homeland.*', 'family', 'families', 'familial', 'group', r'loyal.*', 'rpatriot.*', 'communal', r'commune.*', r'communit.*', r'communis.*', r'comrad.*',\n                  'cadre', r'collectiv.*', 'joint', 'unison', r'unite.*', r'fellow.*', 'guild', 'solidarity', r'devot.*', 'member', r'cliqu.*', 'cohort', 'ally', 'insider', r'segregat.*']\n\ningroup_vice = [r'foreign.*', r'enem.*', r'betray.*', r'treason.*', r'traitor.*', r'treacher.*', r'disloyal.*', r'individual.*', 'apostasy', 'tapostate', 'deserted', r'deserter.*', 'deserting',\n                r'deceiv.*', r'jilt.*', 'imposter', 'miscreant', 'spy', 'sequester', 'renegade', r'terroris.*', r'immigra.*', r\"abandon.*\"]\n\nauthority_virtue = [r'obey.*', r'obedien.*', 'duty', 'law', r'lawful.*', r'legal.*', r'duti.*', r'honor.*', 'respect', r'respectful.*', 'respected', 'respects', r'order.*', r'father.*', 'mother',\n                    r'motherl.*','mothering', 'mothers', r'tradition.*', r'hierarch.*', r'authorit.*', 'permit', 'permission', r'status.*', r'rank.*', r'leader.*', 'class', 'bourgeoisie', r'caste.*', \n                    'position', r'complian.*', 'command', 'supremacy', 'control', r'submi.*', r'allegian.*', 'serve', \"preserve\", r'loyal.*'] \n\nauthority_vice = [r'defian.*', r'rebel.*', r'dissent.*', r'subver.*', r'disrespect.*', r'disobe.*', r'sediti.*', r'agitat.*', r'insubordinat.*', r'illegal.*', r'lawless.*', 'insurgent', 'mutinous', \n                  r'defy.*','dissident', 'unfaithful', 'alienate', 'defector', r'heretic.*', 'nonconformist', 'oppose', 'protest', 'refuse', 'denounce', 'remonstrate', r'riot.*', 'obstruct', \n                  r'betray.*', r'treason.*', r'traitor.*', r'treacher.*', r'disloyal.*', 'apostasy', 'tapostate', 'deserted', r'deserter.*', 'deserting']\n\npurity_virtue = ['piety', 'pious', 'purity', r'pure.*',r'clean.*', r'steril.*', r'sacred.*', r'chast.*', 'holy', 'holiness', r'saint.*', r'wholesome.*', r'celiba.*', 'abstention', 'virgin', 'virgins', \n                 'virginity', 'virginal', 'austerity', 'integrity', 'modesty', r'abstinen.*', 'abstemiousness', 'upright', 'limpid', 'unadulterated', 'maiden', 'virtuous', 'refined', r'decen.*', \n                 'immaculate', 'innocent', 'pristine', r'church.*', \"preserve\"]\n\npurity_vice = [r'disgust.*', r'deprav.*', r'disease.*', r'unclean.*', r'contagio.*', r'indecen.*', 'sin', r'sinful.*', r'sinner.*', 'sins', 'sinned', 'sinning', r'slut.*', 'whore', r'dirt.*', 'impiety',\n               'impious', r'profan.*', 'gross', r'repuls.*', r'sick.*', r'promiscu.*', r'lewd.*', r'adulter.*', r'debauche.*', r'defile.*', 'tramp', r'prostitut.*', 'unchaste', 'intemperate', \n               'wanton', 'profligate', r'filth.*', 'trashy', r'obscen.*', 'lax', r'taint.*', r'stain.*', r'tarnish.*', r'debase.*', r'desecrat.*', r'wicked.*', 'blemish', r'exploitat.*', 'pervert',\n               r'wretched.*', r\"ruin.*\", \"exploit\", \"exploits\", \"exploited\", \"exploiting\", 'apostasy', 'tapostate']\n\nmorality_general = [r'righteous.*', r'moral.*', r'ethic.*', r'value.*', 'upstanding', 'good', 'goodness', r'principle.*', 'blameless', 'exemplary', 'lesson', 'canon', 'doctrine', 'noble', r'worth.*', \n                    r'ideal.*', 'praiseworthy', 'commendable', 'character', 'proper', 'laudable', 'correct', r'wrong.*', 'evil', r'immoral.*', 'bad', r'offend.*', r'offensive.*', r'transgress.*', \n                    r'honest.*', r'lawful.*', r'legal.*', 'pious', 'purity', 'integrity', r'wicked.*', 'upright', r'decen.*', r'indecen.*', r'wretched.*', r'wholesome.*']\n\nlist_of_categories = [harm_virtue, harm_vice, fairness_virtue, fairness_vice, \n                       ingroup_virtue, ingroup_vice, authority_virtue, authority_vice,\n                       purity_virtue, purity_vice, morality_general]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## count frequencies in processed tweets, append each as column\n\napply_counter1 = 0\n\ntweets_processed[\"frequencies\"] = tweets_processed[\"Text_processed\"].apply(\n    compute_frequencies, list_of_categories=list_of_categories\n)\n\nharm_virtue_list2 = []\nharm_vice_list2 = []\nfairness_virtue_list2 = []\nfairness_vice_list2 = []\nauthority_virtue_list2 = []\nauthority_vice_list2 = []\ningroup_virtue_list2 = []\ningroup_vice_list2 = []\npurity_virtue_list2 = []\npurity_vice_list2 = []\nmorality_general_list2 = []\n\n\nlist_of_categories2 = [harm_virtue_list2, harm_vice_list2, fairness_virtue_list2, fairness_vice_list2,\nauthority_virtue_list2, authority_vice_list2, ingroup_virtue_list2, ingroup_vice_list2,\npurity_virtue_list2, purity_vice_list2, morality_general_list2]\n\n\nfor index, row in tweets_processed.iterrows():\n    list_of_frequencies = row[\"frequencies\"]\n\n    for category, frequency in zip(list_of_categories2, list_of_frequencies):\n        category += [frequency]\n\n# ... and then we write each list to a column\nlist_of_category_names = [\"harm_virtue\", \"harm_vice\", \"fairness_virtue\", \"fairness_vice\",\n                          \"ingroup_virtue\", \"ingroup_vice\", \"authority_virtue\", \"authority_vice\",\n                          \"purity_virtue\", \"purity_vice\", \"morality_general\"]\n\nfor cat_name, cat_freqs in zip(list_of_category_names, list_of_categories2):\n    tweets_processed[cat_name] = cat_freqs\n\ndel tweets_processed[\"frequencies\"]\n\ntweets_processed[\"harm_virtue\"]\n\n\ntweets_processed.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## export to csv\n\n\nos.chdir(\"C:/Users/Alex/Desktop/Uni/Master Psychologie/WS 22/SE CSS/Final Project/project/data\")\n\ntweets_processed.to_csv(\"tweets_outcomes.csv\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}